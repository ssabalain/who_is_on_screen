{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b69f39",
   "metadata": {},
   "source": [
    "## Testing connection between Jupyter and the Spark Cluster\n",
    "\n",
    "The main goal of this notebook is to test if the connection between the Jupyter service and the Spark Cluster is working properly.\n",
    "This could be used as a snippet for any other Jupyter-Spark development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa4b6dd",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17bc158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/site-packages (1.3.5)\n",
      "Requirement already satisfied: numpy>=1.19.2 in /usr/local/lib/python3.7/site-packages (from pandas) (1.21.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "546e91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c8d426",
   "metadata": {},
   "source": [
    "### Connecting to Spark cluster and raising a Spark Session\n",
    "\n",
    "Once the session is raised, we should be able to see the app name in http://localhost:8080/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a077ae75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/08/12 10:38:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.2.0\n"
     ]
    }
   ],
   "source": [
    "master = \"spark://spark-master:7077\"\n",
    "app_name = \"Testing if jupyter can communicate with spark\"\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(app_name)\n",
    "    .master(master)\n",
    "    .config(\"spark.driver.memory\", \"512m\")\n",
    "    .config(\"spark.driver.cores\", \"1\")\n",
    "    .config(\"spark.executor.memory\", \"512m\")\n",
    "    .config(\"spark.executor.cores\", \"1\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"Spark version: \" + str(sc.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f0386a",
   "metadata": {},
   "source": [
    "### Reading a tsv file\n",
    "\n",
    "We are going to read a tsv file and compare the reading time with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbfa81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = \"/opt/workspace/src/datasets/imdb_datasets/\"\n",
    "tsv_file = \"name.basics.tsv.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e6f033",
   "metadata": {},
   "source": [
    "#### Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74519cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading csv with Pandas...\")\n",
    "starttime = time.time()\n",
    "df = pd.read_csv(datasets_path + tsv_file,header= 1,compression='gzip', sep ='\\t')\n",
    "endtime = time.time()\n",
    "exec_time = str(endtime - starttime)\n",
    "\n",
    "print(f\"File {tsv_file} succesfully read and loaded as a Pandas dataframe in {exec_time} seconds.\")\n",
    "print(f\"Counting the amount of records in {tsv_file}\")\n",
    "\n",
    "starttime = time.time()\n",
    "total_records = df.count()\n",
    "endtime = time.time()\n",
    "exec_time = str(endtime - starttime)\n",
    "\n",
    "print(f\"Counting finished in {exec_time} seconds. Total amount of records is {total_records}. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f96d6a",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading csv with Spark...\")\n",
    "starttime = time.time()\n",
    "df = spark.read.csv(datasets_path + tsv_file,header= True, sep =r'\\t')\n",
    "endtime = time.time()\n",
    "exec_time = str(endtime - starttime)\n",
    "\n",
    "print(f\"File {tsv_file} succesfully read and loaded as a Spark dataframe in {exec_time} seconds.\")\n",
    "print(f\"Counting the amount of records in {tsv_file}\")\n",
    "\n",
    "starttime = time.time()\n",
    "total_records = df.count()\n",
    "endtime = time.time()\n",
    "exec_time = str(endtime - starttime)\n",
    "\n",
    "print(f\"Counting finished in {exec_time} seconds. Total amount of records is {total_records}. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f1f292",
   "metadata": {},
   "source": [
    "### Terminating Spark session\n",
    "\n",
    "Otherwise, it will be endlessly running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb9e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
