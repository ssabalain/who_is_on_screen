version: "3.7"
services:  
    docs:
      container_name: docs
      networks:
        - default_net
      image: nginx
      ports:
        - "80:80"
      volumes:
        - "./nginx/html:/usr/share/nginx/html:ro"

    mysql:
      container_name: mysql
      restart: always
      networks:
        - default_net
      image: mysql/mysql-server:latest
      ports:
        - '3306:3306'
      environment:
        MYSQL_ROOT_PASSWORD: Whoisonscreen_Root!
        MYSQL_USER: WIOS_User
        MYSQL_PASSWORD: Whoisonscreen!
        MYSQL_DATABASE: facial_db
        MYSQL_ALLOW_EMPTY_PASSWORD: 'yes'
      volumes:
        #- ./facial_database/sql_data/db:/var/lib/mysql
        - mysql-data:/var/lib/mysql

    phpmyadmin:
      container_name: phpmyadmin
      image: phpmyadmin/phpmyadmin:5.0.1
      networks:
        - default_net
      restart: always
      environment:
        PMA_HOST: mysql
        PMA_USER: WIOS_User
        PMA_PASSWORD: Whoisonscreen!
      ports:
        - "8079:80"

    postgres:
        image: postgres:9.6
        container_name: airflow-postgres
        networks:
            - default_net
        volumes: 
            # Create Test database on Postgresql
            - ./docker-airflow/pg-init-scripts:/docker-entrypoint-initdb.d
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
        ports:
            - "5432:5432"

    airflow-webserver:
        image: docker-airflow-spark:1.10.7_3.1.2
        container_name: airflow-webserver
        restart: always
        networks:
            - default_net
        depends_on:
            - postgres
        environment:
            - LOAD_EX=n
            - EXECUTOR=Local
        volumes:
            - ./airflow/dags:/usr/local/airflow/dags #DAG folder
            - ./spark/scripts:/usr/local/spark/app #Spark Scripts (Must be the same path in airflow and Spark Cluster)
            - ./spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)
            - ./facial_database:/usr/local/facial_database #(Must be the same path in airflow and Spark Cluster)
        ports:
            - "8282:8282"
        command: webserver
        healthcheck:
            test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3

    spark:
        image: bitnami/spark:3.1.2
        container_name: spark-master
        user: root # Run container as root container: https://docs.bitnami.com/tutorials/work-with-non-root-containers/
        hostname: spark
        networks:
            - default_net
        environment:
            - SPARK_MODE=master
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ./spark/scripts:/usr/local/spark/app #Spark Scripts (Must be the same path in airflow and Spark Cluster)
            - ./spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)
            - ./facial_database:/usr/local/facial_database #(Must be the same path in airflow and Spark Cluster)
        ports:
            - "8181:8080"
            - "7077:7077"

    spark-worker-1:
        image: bitnami/spark:3.1.2
        container_name: spark-worker-1
        user: root
        networks:
            - default_net
        environment:
            - SPARK_MODE=worker
            - SPARK_MASTER_URL=spark://spark:7077
            - SPARK_WORKER_MEMORY=2G
            - SPARK_WORKER_CORES=2
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ./spark/scripts:/usr/local/spark/app #Spark Scripts (Must be the same path in airflow and Spark Cluster)
            - ./spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)
            - ./facial_database:/usr/local/facial_database #(Must be the same path in airflow and Spark Cluster)

    spark-worker-2:
        image: bitnami/spark:3.1.2
        container_name: spark-worker-2
        user: root
        networks:
            - default_net
        environment:
            - SPARK_MODE=worker
            - SPARK_MASTER_URL=spark://spark:7077
            - SPARK_WORKER_MEMORY=2G
            - SPARK_WORKER_CORES=2
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ./spark/scripts:/usr/local/spark/app #Spark Scripts (Must be the same path in airflow and Spark Cluster)
            - ./spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)
            - ./facial_database:/usr/local/facial_database #(Must be the same path in airflow and Spark Cluster)

    spark-worker-3:
        image: bitnami/spark:3.1.2
        container_name: spark-worker-3
        user: root
        networks:
            - default_net
        environment:
            - SPARK_MODE=worker
            - SPARK_MASTER_URL=spark://spark:7077
            - SPARK_WORKER_MEMORY=2G
            - SPARK_WORKER_CORES=2
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ./spark/scripts:/usr/local/spark/app #Spark Scripts (Must be the same path in airflow and Spark Cluster)
            - ./spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)
            - ./facial_database:/usr/local/facial_database #(Must be the same path in airflow and Spark Cluster)
    
    spark-worker-4:
        image: bitnami/spark:3.1.2
        container_name: spark-worker-4
        user: root
        networks:
            - default_net
        environment:
            - SPARK_MODE=worker
            - SPARK_MASTER_URL=spark://spark:7077
            - SPARK_WORKER_MEMORY=2G
            - SPARK_WORKER_CORES=2
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ./spark/scripts:/usr/local/spark/app #Spark Scripts (Must be the same path in airflow and Spark Cluster)
            - ./spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)
            - ./facial_database:/usr/local/facial_database #(Must be the same path in airflow and Spark Cluster)

    #Jupyter notebook
    jupyter-spark:
        image: jupyter/pyspark-notebook:spark-3.1.2
        container_name: jupyter
        networks:
            - default_net
        ports:
          - "8888:8888"
          - "4040-4080:4040-4080"
        volumes:
          - ./facial_database/jupyter_notebooks:/home/jovyan/work/notebooks/
          - ./spark/resources/data:/home/jovyan/work/data/
          - ./spark/resources/jars:/home/jovyan/work/jars/

volumes:
  postgres-db-volume:
  mysql-data:

networks:
    default_net:







# ---
# version: '3'
# x-airflow-common:
#   &airflow-common
#   image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.0.1}
#   environment:
#     &airflow-common-env
#     AIRFLOW__CORE__EXECUTOR: CeleryExecutor
#     AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
#     AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
#     AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
#     AIRFLOW__CORE__FERNET_KEY: ''
#     AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
#     AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
#     # AIRFLOW__WEBSERVER__WEB_SERVER_PORT: 9090
#   volumes:
#     - ./airflow/dags:/opt/airflow/dags
#     - airflow-logs:/opt/airflow/logs
#     - ./airflow/plugins:/opt/airflow/plugins
#     - ./facial_database:/usr/local/facial_database
#   user: "${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-50000}"
#   depends_on:
#     redis:
#       condition: service_healthy
#     postgres:
#       condition: service_healthy

# services:



#   redis:
#     container_name: redis
#     image: redis:latest
#     ports:
#       - 6379:6379
#     healthcheck:
#       test: ["CMD", "redis-cli", "ping"]
#       interval: 5s
#       timeout: 30s
#       retries: 50
#     restart: always

#   airflow-webserver:
#     <<: *airflow-common
#     container_name: airflow-webserver
#     command: webserver
#     ports:
#       - 9090:8080
#     healthcheck:
#       test: ["CMD", "curl", "--fail", "http://localhost:9090/health"]
#       interval: 10s
#       timeout: 10s
#       retries: 5
#     restart: always

#   airflow-scheduler:
#     <<: *airflow-common
#     container_name: airflow-scheduler
#     command: scheduler
#     restart: always

#   airflow-worker:
#     <<: *airflow-common
#     container_name: airflow-worker
#     command: celery worker
#     restart: always

#   airflow-init:
#     <<: *airflow-common
#     container_name: airflow-init
#     command: version
#     environment:
#       <<: *airflow-common-env
#       _AIRFLOW_DB_UPGRADE: 'true'
#       _AIRFLOW_WWW_USER_CREATE: 'true'
#       _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
#       _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}

#   flower:
#     <<: *airflow-common
#     container_name: flower
#     command: celery flower
#     ports:
#       - 5555:5555
#     healthcheck:
#       test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
#       interval: 10s
#       timeout: 10s
#       retries: 5
#     restart: always

# volumes:
#   postgres-db-volume:
#   mysql-data:
#   airflow-logs:

# # networks:
# #   default_net: