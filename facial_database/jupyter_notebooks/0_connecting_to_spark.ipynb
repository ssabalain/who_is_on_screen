{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef0fd2d7",
   "metadata": {},
   "source": [
    "## Testing connection between Jupyter and the Spark Cluster\n",
    "\n",
    "The main goal of this notebook is to test if the connection between the Jupyter service and the Spark Cluster is working properly.\n",
    "This could be used as a snippet for any other Jupyter-Spark development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38ae071",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebb3beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86bdc115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe66351",
   "metadata": {},
   "source": [
    "### Connecting to Spark cluster and raising a Spark Session\n",
    "\n",
    "Once the session is raised, we should be able to see the app name in http://localhost:8080/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9749b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "master = \"spark://spark-master:7077\"\n",
    "app_name = \"Testing if jupyter can communicate with spark\"\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(app_name)\n",
    "    .master(master)\n",
    "    .config(\"spark.driver.memory\", \"512m\")\n",
    "    .config(\"spark.driver.cores\", \"1\")\n",
    "    .config(\"spark.executor.memory\", \"512m\")\n",
    "    .config(\"spark.executor.cores\", \"1\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"Spark version: \" + str(sc.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d2113c",
   "metadata": {},
   "source": [
    "### Reading a tsv file\n",
    "\n",
    "We are going to read a tsv file and compare the reading time with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14782a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = \"/opt/workspace/facial_database/datasets/imdb_datasets/\"\n",
    "tsv_file = \"name.basics.tsv.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96042531",
   "metadata": {},
   "source": [
    "#### Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484a18c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading csv with Pandas...\")\n",
    "starttime = time.time()\n",
    "df = pd.read_csv(datasets_path + tsv_file,header= 1, sep ='\\t',engine='python', quotechar='\"', on_bad_lines='skip')\n",
    "endtime = time.time()\n",
    "exec_time = str(endtime - starttime)\n",
    "\n",
    "print(f\"File {tsv_file} succesfully read and loaded as a Pandas dataframe in {exec_time} seconds.\")\n",
    "print(f\"Counting the amount of records in {tsv_file}\")\n",
    "\n",
    "starttime = time.time()\n",
    "total_records = df.count()\n",
    "endtime = time.time()\n",
    "exec_time = str(endtime - starttime)\n",
    "\n",
    "print(f\"Counting finished in {exec_time} seconds. Total amount of records is {total_records}. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245aff8e",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731cb88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading csv with Spark...\")\n",
    "starttime = time.time()\n",
    "df = spark.read.csv(datasets_path + tsv_file,header= True, sep =r'\\t')\n",
    "endtime = time.time()\n",
    "exec_time = str(endtime - starttime)\n",
    "\n",
    "print(f\"File {tsv_file} succesfully read and loaded as a Spark dataframe in {exec_time} seconds.\")\n",
    "print(f\"Counting the amount of records in {tsv_file}\")\n",
    "\n",
    "starttime = time.time()\n",
    "total_records = df.count()\n",
    "endtime = time.time()\n",
    "exec_time = str(endtime - starttime)\n",
    "\n",
    "print(f\"Counting finished in {exec_time} seconds. Total amount of records is {total_records}. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6bd57",
   "metadata": {},
   "source": [
    "### Terminating Spark session\n",
    "\n",
    "Otherwise, it will be endlessly running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262788e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
